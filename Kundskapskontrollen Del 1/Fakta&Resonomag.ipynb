{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42727cf8",
   "metadata": {},
   "source": [
    "## Kapitel 1 - Introduktion till maskininl√§rning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8427ed0e",
   "metadata": {},
   "source": [
    "#### 1. Hur h√§nger AI, ML och DL ihop?\n",
    "AI, ML, DL:\n",
    "AI √§r breda tekniker f√∂r att g√∂ra system ‚Äúintelligenta‚Äù.\n",
    "ML √§r en AI-gren d√§r modeller l√§rs fr√•n data.\n",
    "DL √§r en ML-underdel som anv√§nder djupa neuronn√§t f√∂r komplexa uppgifter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961b60e5",
   "metadata": {},
   "source": [
    "#### 2. Vilka √§r de fyra problemkategorierna inom ML?\n",
    "Klassificering, regression, klustring, rekommendation/associationsanalys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5faec552",
   "metadata": {},
   "source": [
    "#### 3. F√∂rklara f√∂ljande:\n",
    "#### a) Vad √§r syftet med att dela upp data i tr√§ningsdata, valideringsdata och testdata?\n",
    "Tr√§ning passar modellen, validering justerar hyperparametrar och tidig stoppning, test ger opartisk uppskattning \n",
    "\n",
    "#### b) Vad √§r k-delad korsvalidering f√∂r n√•got?\n",
    "Dela datan i k lika foldar; tr√§na p√• k-1, validera p√• 1, rotera och medelv√§rdesbilda resultat f√∂r robustare skattning.\n",
    "\n",
    "#### c) Vad √§r root mean squared error (RMSE) f√∂r n√•got?\n",
    "Roten ur medelv√§rdet av kvadrerade fel: RMSE = Straffar stora fel extra och m√§ts i samma enhet som m√•let.\n",
    "\n",
    "#### d) Vad √§r en hyperparameter f√∂r n√•got? Vad √§r en parameter f√∂r n√•got?\n",
    "Hyperparametrar styr inl√§rningsprocessen (t.ex. antal tr√§d, regulariseringsstyrka); parametrar √§r modellens l√§rda vikter.\n",
    "\n",
    "#### e) Vad √§r grid search och hur h√§nger namnet ‚Äúgrid‚Äù och ‚Äúsearch‚Äù ihop med processen som genomf√∂rs? L√§ser vi dokumentationen: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV s√• ser vi att hyperparametern refit har standardv√§rdet True, vad inneb√§r det?\n",
    "Grid search provar alla kombinationer av hyperparametrar i ett rutn√§t (‚Äúgrid‚Äù), utv√§rderar (‚Äúsearch‚Äù) och v√§ljer b√§sta. refit=True betyder att modellen tr√§nas om p√• hela tr√§ningsdatan med de b√§sta hyperparametrarna s√• att vi kan anv√§nda best_estimator_.\n",
    "\n",
    "#### f) Vad √§r kategorisk data och hur hanteras det? I ditt svar, anv√§nd begreppen nominal data, ordinal data, one-hot-encoding, dummy-variable-encoding och ordinal encoding.\n",
    "Nominal (ingen ordning, t.ex. f√§rg)\n",
    "one-hot-encoding eller dummy-variable-encoding (en kolumn kan droppas f√∂r att undvika full kollinearitet). Ordinal (naturlig ordning, t.ex. ‚Äúl√•g/medel/h√∂g‚Äù\n",
    "ordinal encoding som mappar till rangtal.\n",
    "(Rangtal = s√§tt att ordna data, fr√•n l√§gst till h√∂gst (eller tv√§rtom), d√§r det minsta v√§rdet f√•r rang 1, n√§st minst 2, och s√• vidare.)\n",
    "\n",
    "#### g) Vad √§r feature engineering f√∂r n√•got?\n",
    "Skapa eller g√∂ra om variabler f√∂r att f√∂rb√§ttra modeller.\n",
    "\n",
    "#### h) Vad menas med principle of parsimony?\n",
    "V√§lj enklaste modell som f√∂rklarar datan tillr√§ckligt v√§l.. undvik on√∂dig komplexitet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4a9088",
   "metadata": {},
   "source": [
    "#### 4. Vad menas med att ‚Äúen modell √§r en f√∂renkling av verkligheten‚Äù?\n",
    "Den tar bort m√•nga detaljer i verkligheten och f√•ngar bara m√∂nster som finns i tillg√§nglig data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e808406a",
   "metadata": {},
   "source": [
    "#### 5. Vad menas med att en modell √§r ‚Äú√∂veranpassad‚Äù eller overfitted p√• engelska?\n",
    "Presterar s√§mre p√• ny data, h√∂g tr√§ff p√• tr√§ning, l√•g p√• test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b9907d",
   "metadata": {},
   "source": [
    "#### 6. H√∂gre √§r b√§ttre i scikit-learn scoring, vad inneb√§r det?\n",
    "Scorer √§r definierade s√• att st√∂rre v√§rde betyder b√§ttre. F√∂r felm√•tt anv√§nds ofta negativa v√§rden (neg RMSE) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d48b14b",
   "metadata": {},
   "source": [
    "#### 7. Vad √§r tv√§rsnittsdata, tidsseriedata och paneldata? Exemplifiera n√§r respektive datakategori kan uppst√•.\n",
    "Tv√§rsnittsdata flera enheter vid en tidpunkt (elpriser under √•r 2025).\n",
    "Tidsseriedata en enhet √∂ver tid (dagliga bilpriser). \n",
    "Paneldata flera enheter √∂ver tid (kvartalsvisa f√∂rs√§ljningssiffror per butik)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ab6036",
   "metadata": {},
   "source": [
    "#### 8. Ge n√•gra exempel p√• verkliga till√§mpningsomr√•den inom ML. S√∂k g√§rna p√• n√§tet f√∂r att besvara fr√•gan.\n",
    "H√§lso- och sjukv√•rd: bilddiagnostik (t.ex. cancerdetektion), triagering och riskprediktion f√∂r √•terinl√§ggning.\n",
    "Finansiella tj√§nster: kreditbed√∂mning, bedr√§geriidentifiering, algoritmisk handel och kundbortfallsprognoser.\n",
    "Transport och logistik: ruttoptimering, ETA-prognoser, efterfr√•geprognoser och autonom k√∂rning.\n",
    "Handel/marknadsf√∂ring: rekommendationssystem, dynamisk priss√§ttning och lageroptimering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1b2216",
   "metadata": {},
   "source": [
    "#### 9. Generellt sett g√§ller det att h√∂gre √§r b√§ttre i scikit-learn scoring, d√§rf√∂r anv√§nds exempelvis scoring='neg_mean_squared_error'. F√∂rklara logiken bakom detta, det vill s√§ga att vi anv√§nder ‚Äúnegative‚Äù mean squared error.\n",
    "scikit-learns scorer-konvention √§r att ‚Äúst√∂rre v√§rde √§r b√§ttre‚Äù. Felm√•tt som MSE blir d√• inverterade till negativa tal s√• att l√§gre fel motsvarar h√∂gre (mindre negativt) score.\n",
    "Exempel: tv√• modeller med MSE 4 och 9 f√•r scorer -4 och -9; -4 > -9, allts√• v√§ljs modellen med l√§gre MSE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b8b6c4",
   "metadata": {},
   "source": [
    "## Kapitel 2 - Ett ML projekt fr√•n b√∂rjan till slut"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3138615c",
   "metadata": {},
   "source": [
    "#### 1. checklista med sju steg. Beskriv de sju stegen √∂versiktligt. I verkligheten, f√∂ljs dessa steg i en rak progression eller arbetar man generellt sett mer iterativt?\n",
    "1) Problemformulering och m√•l. \n",
    "2) Datainsamling. \n",
    "3) Dataf√∂rberedelse (rensning, hantering av saknade v√§rden, kodning, skalning). \n",
    "4) Explorativ analys och feature engineering. \n",
    "5) Modellval och tr√§ning (inkl. hyperparametertuning). \n",
    "6) Utv√§rdering/validering. \n",
    "7) Deployment/√∂vervakning. Loopar tillbaka n√§r insikter eller driftkrav kr√§ver justeringar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc7171a",
   "metadata": {},
   "source": [
    "#### 2. Vad menas med att en modell produktionss√§tts?\n",
    "Att modellen g√∂rs tillg√§nglig i en driftmilj√∂ d√§r den kan anv√§ndas av riktiga anv√§ndare eller system (API, batchjobb, app), med rutiner f√∂r skalning, s√§kerhet, √∂vervakning och uppdatering.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3f975c",
   "metadata": {},
   "source": [
    "#### 3. Vad √§r scikit-learn f√∂r n√•got? Biblioteket f√∂ljer n√•gra centrala designprinciper. Vilka √§r dessa? Vad √§r estimators, predictors och transformers?\n",
    "scikit-learn √§r ett Pythonbibliotek f√∂r ML med enhetligt API. Designprinciper: konsekvent API (`fit`, `transform`, `predict`), kompositionsbarhet (pipelines), rimliga standarder (sane defaults), tydlig separation mellan hyperparametrar och l√§rda parametrar, samt verktyg f√∂r validering och s√∂k (CV, grid/random search). Estimators: objekt som kan `fit` data. Predictors: estimators med `predict`/`predict_proba`. Transformers: estimators med `transform`/`fit_transform` som √§ndrar features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1885a2e",
   "metadata": {},
   "source": [
    "#### 4. Vad √§r TensorFlow och Keras?\n",
    "TensorFlow √§r ett ramverk f√∂r numeriska ber√§kningar och djupinl√§rning med st√∂d f√∂r CPU/GPU/TPU och deployment (serving, mobile, edge). Keras √§r ett h√∂gre API-lager ovanp√• TensorFlow (default) f√∂r att bygga och tr√§na neuronn√§t enklare, sekventiellt eller funktionellt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa8698c",
   "metadata": {},
   "source": [
    "#### 5. Kalle och Stina diskuterar maskininl√§rning √∂ver en lunch. Kalle s√§ger ‚Äúom jag tr√§nat en modell och den inte presterar bra nog p√• testdatan s√• justerar jag den tills den g√∂r det.‚Äù Stina s√§ger ‚Äúdet √§r ett stort fel att g√∂ra s√•, det enda du d√• √•stadkommer √§r att du √∂veranpassar testdatan. Hela syftet med testdatan f√∂rsvinner d√•‚Äù. Vad s√§ger du om deras dialog?\n",
    "Testdatan ska vara helt oberoende tills alla modell- och hyperparameterbeslut √§r klara. Att iterera mot testresultat g√∂r att testet slutar vara en opartisk generaliseringskoll. Man riskerar \"overfitt\" till just det testsetet. Man ska i st√§llet anv√§nda valideringsdata eller korsvalidering f√∂r justeringar, och spara testsetet till absolut sist.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a19366",
   "metadata": {},
   "source": [
    "#### 6. M√•nga AI/ML-projekt uppn√•r inte m√•len eller l√§mnar aldrig prototypstadiet. Vad beror det p√• och hur b√∂r vi f√∂rh√•lla oss?\n",
    "Vanliga orsaker: otydligt eller r√∂rligt m√•l, bristande eller d√•lig data, felvalda m√§tetal.\n",
    "\n",
    "F√∂rh√•llningss√§tt: l√•s m√•l och m√§tetal tidigt, g√∂r datakvalitetsarbete f√∂rst och h√•ll produkt√§gare l√∂pande."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39a9345",
   "metadata": {},
   "source": [
    "## Kapitel 3 - Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7702b073",
   "metadata": {},
   "source": [
    "#### 1. Vad k√§nnetecknar regressionsproblem? Ge n√•gra exempel p√• till√§mpningsomr√•den.\n",
    "Regression f√∂ruts√§ger ett kontinuerligt numeriskt v√§rde (inte kategorier). Exempel: bosprisf√∂ruts√§gelse (baserat p√• storlek, l√§ge), elprisf√∂ruts√§gelse, temperaturbortfall, b√∂rskursf√∂ruts√§gelse, leveranstidsprognos, h√§lsoriskpo√§ng."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1b4c88",
   "metadata": {},
   "source": [
    "#### 2. F√∂rklara utv√§rderingsm√•tten RMSE, MSE och MAE.\n",
    "MSE (Mean Squared Error): medelv√§rde av kvadrerade fel, $\\text{MSE} = \\frac{1}{n}\\sum(y_i - \\hat{y}_i)^2$. Straffar stora avvikelser extra.\n",
    "\n",
    "RMSE (Root Mean Squared Error): roten ur MSE, $\\text{RMSE} = \\sqrt{\\text{MSE}}$. Samma enhet som m√•lvariabel, tolkbart.\n",
    "\n",
    "MAE (Mean Absolute Error): medelv√§rde av absoluta fel, $\\text{MAE} = \\frac{1}{n}\\sum|y_i - \\hat{y}_i|$. Robust mot extremv√§rden, linj√§r bestrafning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257e7e3c",
   "metadata": {},
   "source": [
    "#### 3. Om vi ska rangordna olika modeller, spelar det n√•gon roll om RMSE eller MSE anv√§nds? Varf√∂r?\n",
    "Nej, det spelar ingen roll. Eftersom $\\text{RMSE} = \\sqrt{\\text{MSE}}$ och kvadratroten √§r en monoton stegvis v√§xande funktion, rangordningen av modeller f√∂rblir densamma. Den modell med l√§gst MSE har ocks√• l√§gst RMSE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83410f35",
   "metadata": {},
   "source": [
    "#### 4. F√∂rklara mycket √∂versiktligt vad gradient descent √§r.\n",
    "Gradient descent √§r en optimeringsalgoritm som minimerar en f√∂rlustfunktion genom att gradvis uppdatera modellens parametrar i riktningen motsatt gradienten (brantaste nedstigningen). Startar fr√•n slumpm√§ssiga parametrar och justerar iterativt f√∂r att n√• ett l√§gre v√§rde p√• f√∂rlusten."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6b6e5d",
   "metadata": {},
   "source": [
    "#### 5. Vad √§r the bias variance trade-off? Varf√∂r √§r mer komplexa modeller inte alltid b√§ttre?\n",
    "Bias √§r felet fr√•n att anta ett f√∂r f√∂renklat samband; variance √§r k√§nsligheten f√∂r sm√• variationer i tr√§ningsdata. Enkla modeller har h√∂gt bias (kan inte f√•nga m√∂nstret) men l√•gt variance. Komplexa modeller har l√•gt bias men h√∂gt variance (overfit). Totalt test-fel = bias¬≤ + variance + irreducible error. En alltf√∂r komplex modell kan l√§ra brus ist√§llet f√∂r m√∂nster, och presterar s√§mre p√• ny data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb17f51",
   "metadata": {},
   "source": [
    "#### 6. N√•gra vanligt f√∂rekommande modeller f√∂r regressionsproblem √§r enligt nedan. F√∂rklara √∂versiktligt hur respektive modell fungerar.\n",
    "\n",
    "a) Linj√§r regression: anpassar en linj√§r funktion $y = \\beta_0 + \\beta_1 x_1 + ... + \\beta_p x_p$ till data genom att minimera summan av kvadrerade residualer. Enkel, tolkbar, antar linj√§rt samband.\n",
    "\n",
    "b) Ridge regression: linj√§r regression med L2-regularisering (kvadrerad norm av koefficienter l√§ggs till f√∂rlusten). Krymper stora koefficienter f√∂r att minska variance och hantera multikollinearitet.\n",
    "\n",
    "c) Lasso regression: linj√§r regression med L1-regularisering (absolut norm). Kan s√§tta vissa koefficienter exakt till noll, vilket ger automatisk feature selection.\n",
    "\n",
    "d) Elastic net: kombinerar L1 och L2-regularisering (Ridge + Lasso). Balanserar b√•da regulariseringsf√∂rdelarna.\n",
    "\n",
    "e) Support Vector Machines (SVR): hittar en hyperplan som minimerar fel inom en $\\epsilon$-marginal, med m√∂jlighet f√∂r k√§rnor (kernels) f√∂r icke-linj√§r regression. Robust, fungerar bra i h√∂ga dimensioner.\n",
    "\n",
    "f) Beslutstr√§d (Decision Trees): rekursiv uppdelning av feature-utrymmet i rektangul√§ra omr√•den. Enkla, icke-linj√§ra, tolka bara, men l√§tta att overfitta.\n",
    "\n",
    "g) Ensemble learning (Voting, Bagging): kombinerar flera modeller (voting tar medelv√§rde av prediktioner). Minskar variance och kan f√∂rb√§ttra generalisering.\n",
    "\n",
    "h) Random Forest: ensemble av beslutttr√§d tr√§nade p√• slumpm√§ssiga delm√§ngder av data och features. Robust, h√∂ga dimensioner, reducerar overfit genom ensemble."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd322ef",
   "metadata": {},
   "source": [
    "#### 7. Vad menas med white box modeller och black box modeller?\n",
    "White box: modeller vars beslut √§r tolka bara och f√∂rklara bara. \n",
    "\n",
    "Black box: modeller som √§r sv√•ra eller om√∂jliga att f√∂rklara intuitivt. Valet mellan dem beror p√• krav p√• tolkbarhet vs prestanda."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8df572",
   "metadata": {},
   "source": [
    "#### 8. Vad √§r skillnaden mellan bagging och pasting?\n",
    "Bagging: tr√§na flera modeller p√• slumpm√§ssiga delm√§ngder av tr√§ningsdata D√ñM BORTFATTNING.\n",
    "Minskar variance genom diversitet och n√§sta m√∂jlighet f√∂r tr√§ning p√• samma sampel flera g√•nger.\n",
    "\n",
    "Pasting: tr√§na flera modeller p√• slumpm√§ssiga delm√§ngder √ñGA UTAN √ÖTERL√ÑGGNING.\n",
    "Mindre diversitet men snabbare ber√§kning; b√•da √§r ensemble-metoder som anv√§nder voting/medelv√§rde f√∂r slutlig prediktion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfb26f5",
   "metadata": {},
   "source": [
    "#### 9. F√∂rklara hur man kan tolka figur 3.1 (s. 113).\n",
    "Figuren visar en enkel linj√§r regressionsmodell d√§r:\n",
    "\n",
    "- x = √•lder (f√∂rklarande variabel)\n",
    "- y = inkomst (responsvariabel)\n",
    "- De svarta punkterna √§r observerade data (ùë¶ùëñ).\n",
    "- Den bl√• linjen √§r den predikterade linjen:y^‚Äã=Œ∏^0‚Äã+Œ∏^1‚Äãx\n",
    "- ùúÉ^ √§r interceptet (d√§r linjen sk√§r y-axeln).\n",
    "- Œ∏^1 √§r lutningen (hur inkomsten f√∂r√§ndras per √∂kad √•lder).\n",
    "- De r√∂da vertikala linjerna √§r residualer: skillnaden mellan observerat v√§rde yi och predikterat v√§rde ùë¶^ùëñ.\n",
    "\n",
    "Kort F√∂rklaring = Figuren visar hur modellen f√∂rs√∂ker hitta en rak linje som b√§st f√∂rklarar sambandet mellan √•lder och inkomst."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229f1d69",
   "metadata": {},
   "source": [
    "#### 10. F√∂rklara hur man kan tolka figur 3.13 (s. 140). Hur h√§nger den ihop med figur 3.14 (s. 141)?\n",
    "Figur 3.13 (tr√§ddiagram):\n",
    "- Visualiserar ett beslutstr√§d f√∂r regression.\n",
    "- Vid varje nod delas datan upp utifr√•n ett villkor, t.ex.: \"√Ñr X2‚â§0.438?\"\n",
    "- Varje delning g√∂rs f√∂r att minimera felet (h√§r anges t.ex. squared_error).\n",
    "- L√∂ven (nedersta rutorna) visar:\n",
    "- Antal datapunkter (samples) i l√∂vet.\n",
    "- Medelv√§rdet (value) som anv√§nds som f√∂ruts√§gelse f√∂r de datapunkterna.\n",
    "\n",
    "Figur 3.14 (f√§rgkartan):\n",
    "- Visar samma beslutstr√§d projicerat p√• dataplanet (X1, X2).\n",
    "- De olika f√§rgerna representerar de f√∂rutsagda v√§rdena i olika regioner.\n",
    "- Varje rektangul√§rt omr√•de i figuren motsvarar ett blad i tr√§det fr√•n figur 3.13.\n",
    "- De horisontella och vertikala linjerna i figuren representerar gr√§nserna (splits) fr√•n tr√§det.\n",
    "\n",
    "Koppling mellan 3.13 - 3.14 =\n",
    "- 3.13 = logiken bakom uppdelningen (regelverket).\n",
    "- 3.14 = hur dessa regler ‚Äù√∂vers√§tts‚Äù till olika omr√•den i datarummet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a285598",
   "metadata": {},
   "source": [
    "####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f596d2",
   "metadata": {},
   "source": [
    "#### 11. Vi har l√§rt oss utv√§rderingsm√•tten RMSE, MSE och MAE. Ett annat utv√§rderingsm√•tt √§r det som ben√§mns f√∂r determinationskoefficienten eller ùëÖ2. F√∂rklara vad det √§r f√∂r m√•tt.\n",
    "R¬≤ (determinationskoefficienten) √§r ett standardiserat utv√§rderingsm√•tt som m√§ter hur stor andel av variationen i m√•lvariabeln som f√∂rklaras av modellen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efb194d",
   "metadata": {},
   "source": [
    "## Kapitel 4 - Klassificering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114da9e2",
   "metadata": {},
   "source": [
    "#### 1. Vad k√§nnetecknar klassificeringsproblem? Ge n√•gra exempel p√• till√§mpningsomr√•den.\n",
    "- Ett klassificeringsproblem inneb√§r att man vill f√∂ruts√§ga vilken kategori (klass) en observation tillh√∂r, baserat p√• dess egenskaper (features).\n",
    "- M√•let √§r allts√• att tilldela varje datapunkt till en av ett antal diskreta klasser.\n",
    "- Till skillnad fr√•n regression, d√§r man f√∂rutsp√•r ett kontinuerligt v√§rde, √§r klassificering alltid diskret/icke-numerisk (t.ex. ja/nej, hund/katt/f√•gel, riskniv√•er).\n",
    "\n",
    "Till√§mpningsomr√•den:\n",
    "- Medicinsk diagnos: Klassificera om en patient har en viss sjukdom (t.ex. \"cancer\" vs \"inte cancer\").\n",
    "- E-postfilter: Avg√∂ra om ett mejl √§r \"spam\" eller \"inte spam\".\n",
    "- Bildigenk√§nning: Identifiera objekt i bilder, t.ex. \"hund\", \"katt\" eller \"bil\".\n",
    "- Kreditbed√∂mning: F√∂ruts√§ga om en kund √§r en \"l√•g risk\" eller \"h√∂g risk\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e2d71a",
   "metadata": {},
   "source": [
    "#### 2 F√∂rklara hur OvR- och OvO-algoritmerna fungerar.\n",
    "##### Multiklassklassificering: OvR vs. OvO\n",
    "\n",
    "One-vs-Rest (OvR) / One-vs-All F√∂r varje klass bygger man en separat klassificerare som skiljer den klassen fr√•n alla andra.\n",
    "Om man har K klasser tr√§nar man allts√• K modeller.\n",
    "Vid prediktion f√•r man ett resultat fr√•n varje modell och v√§ljer den klass som f√•r h√∂gst sannolikhet/po√§ng.\n",
    "\n",
    "Exempel (K = 3, klasser = {Hund, Katt, F√•gel}):\n",
    "- Modell 1: Hund vs (Katt + F√•gel)\n",
    "- Modell 2: Katt vs (Hund + F√•gel)\n",
    "- Modell 3: F√•gel vs (Hund + Katt)\n",
    "\n",
    "Vid testning v√§ljer man den modell som √§r mest s√§ker.\n",
    "\n",
    "F√∂rdelar:\n",
    "- Relativt effektivt (bara K modeller).\n",
    "- L√§tt att implementera.\n",
    "- Nackdelar:\n",
    "\n",
    "Obalanserade datam√§ngder (‚Äùrest‚Äù √§r ofta mycket st√∂rre √§n den aktuella klassen).\n",
    "Klassificerarna kan bli oj√§mnt starka.\n",
    "One-vs-One (OvO) Man bygger en klassificerare f√∂r varje par av klasser.\n",
    "Om man har K klasser tr√§nar man allts√• K(K‚àí1)/2 modeller.\n",
    "Vid prediktion r√∂star alla modeller, och den klass som f√•r flest r√∂ster v√§ljs.\n",
    "\n",
    "Exempel (K = 3, klasser = {Hund, Katt, F√•gel}):\n",
    "- Modell 1: Hund vs Katt\n",
    "- Modell 2: Hund vs F√•gel\n",
    "- Modell 3: Katt vs F√•gel\n",
    "- Vid testning r√∂star modellerna och majoriteten avg√∂r.\n",
    "\n",
    "F√∂rdelar:\n",
    "- Mindre obalans i data (endast tv√• klasser per modell).\n",
    "- Varje modell √§r enklare att tr√§na.\n",
    "\n",
    "Nackdelar:\n",
    "- M√•nga fler modeller beh√∂vs n√§r K √§r stort (t.ex. v√§ldigt m√•nga vid 100 klasser).\n",
    "- L√§ngre prediktionstid eftersom m√•nga r√∂ster m√•ste ber√§knas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f616c6",
   "metadata": {},
   "source": [
    "#### 3. F√∂rklara f√∂ljande utv√§rderingsm√•tt:\n",
    "a) Confusion Matrix En tabell som visar antalet korrekta och felaktiga prediktioner f√∂r varje klass.\n",
    "Den j√§mf√∂r sanna etiketter (faktiskt utfall) med predikterade etiketter.\n",
    "\n",
    "b) Accuracy Andel korrekta prediktioner av alla fall. Bra n√§r klasserna √§r balanserade, men kan vara missvisande vid obalanserad data.\n",
    "\n",
    "c) Precision Hur stor andel av de predikterade positiva som verkligen √§r positiva.\n",
    "Svarar p√• fr√•gan: ‚ÄùN√§r modellen s√§ger positivt, hur ofta har den r√§tt?‚Äù\n",
    "\n",
    "d) Recall (√§ven Sensitivity eller TPR) Hur stor andel av de faktiska positiva som modellen hittar.\n",
    "Svarar p√• fr√•gan: ‚ÄùHur m√•nga av de verkliga positiva hittar modellen?‚Äù\n",
    "\n",
    "e) F1-score Ett balanserat m√•tt som kombinerar precision och recall.\n",
    "Bra n√§r man vill v√§ga b√•de precision och recall lika.\n",
    "\n",
    "f) ROC-kurvan Receiver Operating Characteristic-kurvan visar sambandet mellan:\n",
    "\n",
    "- True Positive Rate (TPR) = Recall\n",
    "- False Positive Rate (FPR) = FP / (FP + TN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e65927",
   "metadata": {},
   "source": [
    "\n",
    "#### 4. Vad √§r precision-recall tradeoff f√∂r n√•got?\n",
    "\n",
    "N√§r man √§ndrar tr√∂skelv√§rdet (threshold) som modellen anv√§nder f√∂r att avg√∂ra om ett exempel √§r positivt eller negativt, p√•verkar det b√•de precision och recall.\n",
    "\n",
    "- L√•g tr√∂skel √§r fler prediktioner blir positiva\n",
    "\n",
    "- - H√∂g recall (f√§rre falskt negativa)\n",
    "- - L√•g precision (fler falskt positiva)\n",
    "\n",
    "- H√∂g tr√∂skel √§r f√§rre prediktioner blir positiva\n",
    "- - H√∂g precision (f√§rre falskt positiva)\n",
    "- - L√•g recall (fler falskt negativa)\n",
    "\n",
    "Detta kallas precision‚Äìrecall tradeoff, eftersom man ofta m√•ste v√§lja mellan att optimera f√∂r precision eller recall beroende p√• problemet.\n",
    "\n",
    "Exempel p√• anv√§ndning:\n",
    "- Medicinsk diagnostik: Man vill ofta ha h√∂g recall (inte missa sjuka patienter)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ef5b11",
   "metadata": {},
   "source": [
    "#### 5. Vanliga modeller f√∂r klassificering\n",
    "a) Logistisk regression\n",
    "- Anv√§nds f√∂r bin√§r och multiklassklassificering.\n",
    "- Bygger p√• en linj√§r modell (likt linj√§r regression) men anv√§nder en sigmoid-funktion f√∂r att ge sannolikheter mellan 0 och 1.\n",
    "- Klass = 1 om sannolikheten √∂verstiger ett tr√∂skelv√§rde (t.ex. 0.5).\n",
    "- Tolkningsbar: vikterna visar hur varje feature p√•verkar sannolikheten.\n",
    "\n",
    "b) Support Vector Machines (SVM)\n",
    "- Id√©n √§r att hitta en beslutsgr√§ns (hyperplan) som maximalt separerar klasserna.\n",
    "- Endast de datapunkter som ligger n√§rmast gr√§nsen (support vectors) p√•verkar modellen.\n",
    "- Kan anv√§nda kernel-trick f√∂r att hantera icke-linj√§ra gr√§nser (t.ex. RBF-kernel).\n",
    "- Ofta bra f√∂r sm√• till medelstora dataset med tydlig klass-separation.\n",
    "\n",
    "c) Beslutstr√§d\n",
    "- Delar upp datat i mindre grupper genom att st√§lla fr√•gor (‚Äùom feature < x, g√• v√§nster annars h√∂ger‚Äù).\n",
    "- Bygger en tr√§dstruktur d√§r varje blad motsvarar en klass.\n",
    "- L√§tt att tolka och visualisera.\n",
    "- Nackdel: kan bli √∂veranpassat (overfitting) om tr√§det √§r f√∂r djupt.\n",
    "\n",
    "d) Ensemblemetoder Kombinerar flera modeller f√∂r att f√∂rb√§ttra prestanda.\n",
    "- Voting Classifier:\n",
    "- - Tr√§nar flera olika modeller.\n",
    "- - G√∂r en omr√∂stning (majoritet eller medelv√§rde av sannolikheter).\n",
    "- - Ger ofta b√§ttre resultat √§n en enskild modell.\n",
    "- Bagging Classifier:\n",
    "- - Bygger flera modeller av samma typ p√• olika bootstrap-samplade dataset.\n",
    "- - Sl√•r samman prediktionerna (t.ex. majoritetsr√∂stning).\n",
    "- - Minskar varians och f√∂rb√§ttrar robusthet.\n",
    "\n",
    "e) Random Forest\n",
    "- En ensemble av beslutstr√§d.\n",
    "- Varje tr√§d tr√§nas p√• en slumpm√§ssig delm√§ngd av data + slumpm√§ssigt valda features vid varje split.\n",
    "- Prediktionen g√∂rs genom majoritetsr√∂stning mellan tr√§den.\n",
    "- F√∂rdelar: robust, hanterar stora dataset bra, minskar risken f√∂r √∂veranpassning j√§mf√∂rt med ett enskilt beslutstr√§d.\n",
    "\n",
    "f) Extra Trees (Extremely Randomized Trees)\n",
    "- Liknar Random Forest men g√∂r √§nnu mer slumpm√§ssighet:\n",
    "- V√§ljer inte bara slumpm√§ssiga features, utan √§ven slumpm√§ssiga tr√∂skelv√§rden f√∂r split.\n",
    "- Detta g√∂r tr√§den √§nnu mer varierade och snabbare att tr√§na.\n",
    "- Ofta bra resultat men kan f√∂rlora lite tolkningsbarhet j√§mf√∂rt med vanliga beslutstr√§d."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3479365f",
   "metadata": {},
   "source": [
    "#### 6. Vad inneb√§r det att vi kan kolla p√• feature importance med hj√§lp av tr√§d-modeller s√•som beslutstr√§d eller random forest?\n",
    "Tr√§dmodeller som beslutstr√§d, random forest och extra trees kan ge en uppskattning av hur viktiga olika features √§r f√∂r klassificeringen.\n",
    "\n",
    "Hur fungerar det?\n",
    "- N√§r ett tr√§d g√∂r en split p√• en feature (t.ex. √•lder < 30), ber√§knas hur mycket den splitten minskar os√§kerheten i datat (t.ex. minskning i gini impurity eller entropi).\n",
    "- Ju mer en feature bidrar till att f√∂rb√§ttra klassificeringen (dvs. ger renare noder), desto h√∂gre vikt f√•r den.\n",
    "- I en ensemble (som random forest) ber√§knas medelv√§rdet av alla tr√§dens bidrag.\n",
    "\n",
    "Tolkning\n",
    "- H√∂g feature importance ‚Üí den feature har stor betydelse f√∂r modellens prediktioner.\n",
    "- L√•g feature importance ‚Üí den feature p√•verkar lite eller inte alls.\n",
    "\n",
    "Begr√§nsningar\n",
    "- Feature importance kan bli snedvridet mot features med m√•nga m√∂jliga v√§rden (t.ex. kontinuerliga variabler).\n",
    "- Den visar korrelation, inte kausalitet.\n",
    "- Vid starkt korrelerade features kan importancen f√∂rdelas ‚Äùgodtyckligt‚Äù mellan dem.\n",
    "\n",
    "Anv√§ndning\n",
    "- F√∂r att tolka modellen: vilka variabler driver besluten?\n",
    "- F√∂r feature selection: identifiera och ta bort mindre viktiga variabler."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503fb8a5",
   "metadata": {},
   "source": [
    "#### 7. Precision vs. Recall ‚Äì diskussion\n",
    "Stina vill ha h√∂gsta m√∂jliga precision.\n",
    "Kalle undrar d√•: \"Men vad h√§nder med recall?\"\n",
    "\n",
    "Sambandet\n",
    "- Om vi maximerar precision, blir modellen v√§ldigt f√∂rsiktig med att s√§ga ‚Äùpositivt‚Äù.\n",
    "- - Detta g√∂r att vi f√•r f√§rre falskt positiva.\n",
    "- - Men risken √§r att vi samtidigt missar m√•nga sanna positiva ‚Üí l√§gre recall.\n",
    "- Precision och recall st√•r allts√• ofta i konflikt (tradeoff).\n",
    "\n",
    "N√§r √§r h√∂g precision √∂nskv√§rt?\n",
    "- N√§r det √§r mycket viktigt att ett positivt resultat verkligen √§r korrekt.\n",
    "- Exempel:\n",
    "- - Spamfilter: vi vill inte riskera att legitima mejl hamnar i skr√§pposten.\n",
    "- - R√§ttsv√§sendet: man vill inte d√∂ma en oskyldig (falskt positiv).\n",
    "\n",
    "N√§r kan h√∂g precision vara d√•ligt?\n",
    "- N√§r det √§r viktigare att inte missa n√•gra positiva fall, dvs. n√§r h√∂g recall √§r viktigare.\n",
    "- Exempel:\n",
    "- - Medicinsk screening (t.ex. cancerdiagnos): det √§r v√§rre att missa en sjuk patient (falskt negativ) √§n att ha n√•gra falsklarm (falskt positiva).\n",
    "\n",
    "Precision‚ÄìRecall tradeoff i r√§ttsv√§sendet\n",
    "- I r√§ttssystemet √§r det oftast mycket viktigare med h√∂g precision:\n",
    "- - Det √§r v√§rre att d√∂ma en oskyldig (falskt positiv) √§n att fria en skyldig (falskt negativ).\n",
    "- Detta inneb√§r att man accepterar att recall kan bli l√§gre (alla skyldiga hittas inte), s√• l√§nge man √§r s√§ker p√• att de som d√∂ms verkligen √§r skyldiga.\n",
    "\n",
    "Kort sagt:\n",
    "- H√∂g precision √§r ‚ÄùVi √§r s√§kra p√• att de vi pekar ut √§r skyldiga, men vi kanske missar n√•gra.‚Äù\n",
    "- H√∂g recall √§r ‚ÄùVi f√•ngar n√§stan alla skyldiga, men riskerar att √§ven n√•gra oskyldiga drabbas.‚Äù"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab90593",
   "metadata": {},
   "source": [
    "#### 8. F√∂rklara hur man kan tolka figur 4.8 p√• sidan 175.\n",
    "Figuren visar hur en logistisk regressionsmodell delar upp datarummet i tv√• klasser (klass 0 och klass 1) baserat p√• variablerna X1 och X2.\n",
    "\n",
    "Vad figuren visar\n",
    "- Bakgrundsf√§rgerna representerar modellens predikterade sannolikhet att en punkt tillh√∂r klass 1:\n",
    "- - Lila ‚âà sannolikhet n√§ra 0 (klass 0).\n",
    "- - Gul ‚âà sannolikhet n√§ra 1 (klass 1).\n",
    "- - Mellanf√§rger (gr√∂nt/turkos) ‚âà os√§kerhet (sannolikhet runt 0.5).\n",
    "\n",
    "Cirklarna (datapunkterna):\n",
    "- F√§rgade efter deras verkliga klass (klass 0 eller klass 1).\n",
    "- Man kan se var datapunkterna ligger i f√∂rh√•llande till sannolikhetsytan.\n",
    "- Beslutsgr√§nsen (d√§r sannolikheten ~ 0.5):\n",
    "\n",
    "Syns som √∂verg√•ngen mellan det lila och det gula omr√•det.\n",
    "H√§r v√§ljer modellen mellan klass 0 och klass 1.\n",
    "Tolkning\n",
    "\n",
    "Modellen har hittat en linj√§r beslutsgr√§ns (logistisk regression √§r en linj√§r modell i feature-rummet).\n",
    "- Omr√•den till v√§nster/ovanf√∂r gr√§nsen √§r klassificeras som klass 0.\n",
    "- Omr√•den till h√∂ger/nedanf√∂r gr√§nsen √§r klassificeras som klass 1.\n",
    "- Ju l√§ngre bort fr√•n gr√§nsen, desto mer s√§ker √§r modellen i sin prediktion (sannolikheten g√•r mot 0 eller 1).\n",
    "\n",
    "Slutsats\n",
    "- Figuren visar b√•de klassindelningen (0 eller 1) och os√§kerheten i modellens prediktioner.\n",
    "- Man kan se var modellen g√∂r fel (datapunkter som ligger i \"fel f√§rgat omr√•de\") och var den √§r os√§ker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0d999e",
   "metadata": {},
   "source": [
    "#### 9. Varf√∂r .fit_transform() p√• tr√§ningsdata men bara .transform() p√• validerings- och testdata?\n",
    "Logiken bakom\n",
    "- .fit(): Ber√§knar de parametrar som beh√∂vs fr√•n datat.\n",
    "- - Exempel: StandardScaler r√§knar ut medelv√§rde och standardavvikelse f√∂r varje feature.\n",
    "- .transform(): Anv√§nder de redan ber√§knade parametrarna f√∂r att skala eller transformera ny data.\n",
    "\n",
    "Varf√∂r?\n",
    "- P√• tr√§ningsdata anv√§nder vi .fit_transform() f√∂r att b√•de:\n",
    "- - Ber√§kna parametrar (t.ex. medel och standardavvikelse).\n",
    "- - Skala datat utifr√•n dessa.\n",
    "\n",
    "- P√• validerings- och testdata anv√§nder vi endast .transform():\n",
    "- - Vi f√•r inte ‚Äùtjuvkika‚Äù p√• dessa dataset och l√•ta deras information p√•verka tr√§ningen.\n",
    "- - Om vi skulle k√∂ra .fit_transform() √§ven p√• validerings- eller testdata s√• riskerar dataleakage.\n",
    "- - D√• skulle modellens utv√§rdering bli orealistiskt bra, eftersom den indirekt f√•tt information fr√•n datat den inte ska k√§nna till under tr√§ning.\n",
    "\n",
    "Exemplet med StandardScaler\n",
    "#### Tr√§ningsdata: ber√§kna medel och std, skala\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "\n",
    "##### Validerings- och testdata: anv√§nd samma medel och std som f√∂r tr√§ningsdatan\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbeeeb1",
   "metadata": {},
   "source": [
    "## Kapitel 7 - Artificiella neurala n√§tverk (ANN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b9042f",
   "metadata": {},
   "source": [
    "#### 1. Vad har ANN modellerna inspirerats av?\n",
    "ANN-modeller har inspirerats av den biologiska hj√§rnan och hur biologiska neuroner √§r sammankopplade och kommunicerar med varandra genom elektriska signaler. Strukturen efterliknar hur nervceller (neuroner) tar emot signaler, bearbetar dem och skickar vidare information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cbb739",
   "metadata": {},
   "source": [
    "#### 2. Vad refererar \"djup\" till i begreppet \"djupinl√§rning\"?\n",
    "\"Djup\" refererar till antalet dolda lager (hidden layers) i det neurala n√§tverket. Ett djupt neuralt n√§tverk har flera lager mellan input- och outputlagret, vilket g√∂r att n√§tverket kan l√§ra sig mer komplexa och abstrakta representationer av data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062a6765",
   "metadata": {},
   "source": [
    "#### 3. F√∂rklara figurerna och varf√∂r aktiveringsfunktioner anv√§nds\n",
    "Figur 7.2: Visar en enkel arkitektur med endast ett input-lager och ett output-lager. Inputneuronen (1) och feature-neuronen (X‚ÇÅ) √§r kopplade till outputneuronen via vikter (W‚ÇÅ och W‚ÇÇ). Outputen √§r en linj√§r kombination: W‚ÇÅ + W‚ÇÇX‚ÇÅ. Detta skiljer sig inte fr√•n vanlig linj√§r regression.\n",
    "\n",
    "Figur 7.3: Visar en mer komplex arkitektur med ett dolt lager (hidden layer) mellan input och output. Nu har vi tv√• hidden neurons som tar emot viktade summor fr√•n inputen, och dessa skickas vidare till outputlagret. Detta skapar en hierarkisk struktur som kan l√§ra sig mer komplexa m√∂nster.\n",
    "\n",
    "Figur 7.4: Visar tv√• exempel p√• aktiveringsfunktioner:\n",
    "Logistic/Sigmoid: œÉ(z) = 1/(1+e‚Åª·∂ª) - ger output mellan 0 och 1\n",
    "ReLU: ReLU(z) = max(0, z) - ger 0 f√∂r negativa v√§rden, annars v√§rdet sj√§lvt\n",
    "\n",
    "Figur 7.5: Visar en single-layer perceptron d√§r outputneuronen inneh√•ller b√•de summeringen (Œ£) och en aktiveringsfunktion (f). Detta √§r det som k√§nnetecknar en perceptron - den √§r fully connected/dense och har icke-linj√§ra aktiveringsfunktioner.\n",
    "\n",
    "Varf√∂r aktiveringsfunktioner? Utan aktiveringsfunktioner skulle ett neuralt n√§tverk, oavsett hur m√•nga lager det har, bara kunna l√§ra sig linj√§ra samband (det skulle bara vara en linj√§r kombination av linj√§ra kombinationer). Aktiveringsfunktioner introducerar icke-linj√§ritet vilket g√∂r att n√§tverket kan approximera komplexa, icke-linj√§ra funktioner och l√§ra sig komplicerade m√∂nster i data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b363ec",
   "metadata": {},
   "source": [
    "#### 4. Har neurala n√§tverk f√• eller m√•nga parametrar?\n",
    "Neurala n√§tverk har typiskt m√•nga parametrar. Varje koppling (edge) mellan neuroner representerar en vikt (parameter), och med flera lager och m√•nga neuroner per lager blir antalet parametrar mycket stort. Detta √§r b√•de en styrka (kan l√§ra sig komplexa m√∂nster) och en svaghet (risk f√∂r overfitting, kr√§ver mycket data och ber√§kningskraft)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdb5be0",
   "metadata": {},
   "source": [
    "#### 5. F√∂rklara intuitivt hur dropout-regularisering fungerar\n",
    "Dropout fungerar genom att slumpm√§ssigt \"st√§nga av\" (s√§tta till 0) en andel av neuronerna under tr√§ning. Till exempel, med en dropout-rate p√• 0.5 st√§ngs 50% av neuronerna av slumpm√§ssigt i varje tr√§ningsiteration.\n",
    "\n",
    "Intuitivt: Detta tvingar n√§tverket att inte f√∂rlita sig f√∂r mycket p√• enskilda neuroner eller specifika kombinationer av neuroner. Ist√§llet m√•ste n√§tverket l√§ra sig mer robusta och generella representationer som fungerar √§ven n√§r vissa neuroner √§r borta. Det √§r som att tr√§na ett team d√§r inte alla medlemmar alltid √§r n√§rvarande - teamet m√•ste bli mer flexibelt och varje medlem m√•ste kunna bidra sj√§lvst√§ndigt. Detta minskar overfitting eftersom n√§tverket inte kan \"memorera\" tr√§ningsdata genom specifika neuroner."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca1abee",
   "metadata": {},
   "source": [
    "## Kapitel 8 - Convolutional neural network (CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a48eb86",
   "metadata": {},
   "source": [
    "#### 1. Vad kr√§vs det f√∂r att ett neuralt n√§tverk ska klassas som CNN?\n",
    "Minst ett konvolutionslager (convolution layer) d√§r filtrer/k√§rnor sveps √∂ver indata och delar vikter rumsligt; ofta f√∂ljt av pooling f√∂r nedprovning. Dense lager kan finnas men utan convolution √§r det inte ett CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33af36ef",
   "metadata": {},
   "source": [
    "#### 2. Inom vilket till√§mpningsomr√•de √§r CNN generellt sett en v√§ldigt kraftfull modell?\n",
    "Framf√∂r allt bild- och videorelaterade uppgifter (klassificering, detektion, segmentering) men √§ven ljud/signal- och textbearbetning d√§r det finns lokal struktur."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe7d145",
   "metadata": {},
   "source": [
    "#### 3. Vad menas med RGB?\n",
    "En f√§rgkanalrepresentation med tre komponenter: R√∂d, Gr√∂n och Bl√• (Red, Green, Blue). Varje pixel har tre v√§rden (ofta 0‚Äì255) som kombineras f√∂r att skapa en f√§rg."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670cceda",
   "metadata": {},
   "source": [
    "#### 4. Vad √§r data augmentation?\n",
    "Tekniker som skapar fler varierade tr√§ningsprov fr√•n befintliga data genom transformationer (t.ex. rotation, besk√§rning, spegling, f√§rgskift, brus). √ñkar robusthet och minskar overfitting utan att samla ny data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c134110a",
   "metadata": {},
   "source": [
    "#### 5. F√∂rklara √∂versiktligt hur CNN fungerar. Anv√§nd begreppen convolution layer och pooling layer i ditt svar.\n",
    "CNN bearbetar data lokalt och hierarkiskt: convolution layer applicerar sm√• filter som glider √∂ver indata och detekterar lokala m√∂nster (kanter, texturer). Aktiveringar staplas i feature maps som f√•ngar vad och var n√•got finns. D√§refter minskar pooling layer (t.ex. max pooling) dimensionen genom att sammanfatta lokala regioner, vilket ger viss translationsinvarians och f√§rre parametrar. Djupare lager l√§r mer abstrakta m√∂nster; i slutet anv√§nds ofta fullt kopplade lager f√∂r klassificering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7247744d",
   "metadata": {},
   "source": [
    "#### 6. Din kollega ber dig f√∂rklara figur 8.4. G√∂r det!\n",
    "Figuren visar ett typiskt CNN-fl√∂de f√∂r en 32x32 RGB-bild: f√∂rst konvolutioner som anv√§nder sm√• filter f√∂r att extrahera lokala m√∂nster och generera flera feature maps. Efter varje konv-block f√∂ljer ett 2x2 max-pooling-lager som halverar h√∂jd och bredd och beh√•ller starkaste signalen, vilket minskar dimensioner och g√∂r modellen mer robust mot sm√• f√∂rskjutningar. Antalet filter √∂kar stegvis (t.ex. 8‚Üí16‚Üí32) medan spatiala m√•tten minskar, s√• representationen blir djupare men kompaktare. I slutet plattas (flatten) feature maps ut till en vektor som skickas in i fullst√§ndigt kopplade lager (ex. 500 noder, sedan 100) f√∂r den slutliga klassificeringen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447a676c",
   "metadata": {},
   "source": [
    "## Kapitel 10 - Chattbottar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a9901f",
   "metadata": {},
   "source": [
    "#### 1. Vad √§r prompt engineering?\n",
    "Prompt engineering handlar om att st√§lla \"effektiva fr√•gor\" till chattbottar f√∂r att f√• \"b√§ttre\" svar. Som tumregel √§r det bra att vara s√• specifik, deskriptiv och detaljerad som m√∂jligt om √∂nskad kontext, utfall, l√§ngd, format och stil."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e010e8",
   "metadata": {},
   "source": [
    "\n",
    "#### 2. Vad √§r RAG?\n",
    "En RAG-modell inneh√•ller tv√• delar: en retriever, som s√∂ker efter relevanta stycken i en st√∂rre text. Dessa stycken skickas sedan vidare som kontext till den andra delen, en generator som genererar svaren utifr√•n den givna kontexten.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8016f9f",
   "metadata": {},
   "source": [
    "\n",
    "#### 3. Vad √§r chunking och embeddings?\n",
    "Chunking inneb√§r att dela upp text i mindre delar, och embeddings √§r v√§rdena p√• dessa delar som hittas i embeddings-attributet.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e68eb3a",
   "metadata": {},
   "source": [
    "#### 4.  F√∂rklara √∂versiktligt hur man kan evaluera en chattbot.\n",
    "Automatiska m√•tt: BLEU/ROUGE (ytlikhet), BERTScore/semantic similarity; f√∂r retrieval: grounding/faithfulness, context precision/recall.\n",
    "\n",
    "LLM-as-judge: LLM som graderar svar p√• korrekthet, relevans, s√§kerhet.\n",
    "\n",
    "M√§nsklig utv√§rdering: bed√∂m korrekthet, relevans, koherens, artighet/ton, s√§kerhet (tox, PII), hallucinationer, persona- och policy‚Äëf√∂ljsamhet.\n",
    "\n",
    "Uppgiftsm√•tt (task bots): task success rate, first-turn success, tid till l√∂sning, antal steg, fallback-/eskalationsgrad, containment.\n",
    "\n",
    "UX/Kund: CSAT, NPS, CES, avhopp/frekvens, retention.\n",
    "\n",
    "Experiment: A/B-tester p√• riktiga anv√§ndare; r√∂dteamning/adversarial tests; edge-case suites.\n",
    "\n",
    "Data/Loggar: feltyper, toppintentioner som misslyckas, svarsl√§ngd, latens, load/stabilitet.\n",
    "\n",
    "S√§kerhet: jailbreak- och policytester; eval mot k√§nda risklistor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca72ef5",
   "metadata": {},
   "source": [
    "#### 5. Din kollega fr√•gar dig vad ELIZA och Turingtestet √§r. \n",
    "ELIZA: Tidigt (1966, Weizenbaum) regelbaserat program som med m√∂nstermatchning/paraphrasing imiterade en Rogerian terapeut; ingen verklig f√∂rst√•else.\n",
    "\n",
    "Turingtestet: F√∂reslaget 1950 av Alan Turing; ‚Äúimitation game‚Äù d√§r en domare via text f√∂rs√∂ker avg√∂ra om motparten √§r m√§nniska eller maskin. Klarat test ‚áí maskinen √§r sv√•r att skilja fr√•n m√§nniska. Begr√§nsningar: m√§ter imitation, inte verklig f√∂rst√•else eller generell intelligens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7e1da7",
   "metadata": {},
   "source": [
    "#### 6. Ge konkreta exempel p√• hur olika f√∂retag och organisationer kan ta nytta av chattbottar samt vilka risker som finns.\n",
    "Ett stort flaw var √•r 2025, n√§r n√•gon fr√•gade Playstation 5s nya chatbot n√§r ett spel som inte hade ett release date skulle sl√§ppas och den svarade p√• fr√•gan s√• f√∂retaget var tvungen o sl√§ppa spelet tidigare. Detta √§r ett flaw f√∂r botten hade access till deras hela databas."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
